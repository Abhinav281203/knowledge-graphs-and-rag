{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13ed26c-2b64-4439-9385-4cf90ed3325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in /opt/homebrew/lib/python3.11/site-packages (0.8.50)\n",
      "Requirement already satisfied: nebula3-python in /opt/homebrew/lib/python3.11/site-packages (3.4.0)\n",
      "Requirement already satisfied: futures in /opt/homebrew/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: glob2 in /opt/homebrew/lib/python3.11/site-packages (0.7)\n",
      "Requirement already satisfied: pypdf in /opt/homebrew/lib/python3.11/site-packages (3.16.4)\n",
      "Requirement already satisfied: sentence_transformers in /opt/homebrew/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.11/site-packages (0.0.322)\n",
      "Collecting ipython-ngql\n",
      "  Obtaining dependency information for ipython-ngql from https://files.pythonhosted.org/packages/4a/1a/11cfe058ae84df31f503d115941fe665b116ef6fee3488ba284d03ba45c3/ipython_ngql-0.7.5-py3-none-any.whl.metadata\n",
      "  Downloading ipython_ngql-0.7.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pyvis\n",
      "  Using cached pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (3.2)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (2.0.22)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (0.5.14)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (2023.10.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (1.26.1)\n",
      "Requirement already satisfied: openai>=0.26.4 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (2.1.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2 in /opt/homebrew/lib/python3.11/site-packages (from llama_index) (1.26.18)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in /opt/homebrew/lib/python3.11/site-packages (from nebula3-python) (0.22.0)\n",
      "Requirement already satisfied: future>=0.18.0 in /opt/homebrew/lib/python3.11/site-packages (from nebula3-python) (0.18.3)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/homebrew/lib/python3.11/site-packages (from nebula3-python) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /opt/homebrew/lib/python3.11/site-packages (from nebula3-python) (2023.3.post1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (4.34.1)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (1.11.3)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (0.17.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (0.0.51)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: Jinja2 in /opt/homebrew/lib/python3.11/site-packages (from ipython-ngql) (3.1.2)\n",
      "Collecting ipython>=5.3.0 (from pyvis)\n",
      "  Obtaining dependency information for ipython>=5.3.0 from https://files.pythonhosted.org/packages/ef/02/fc039fca3ec40a00f962eb6e9da45c507334b0650a3cb9facd38d234fb7a/ipython-8.16.1-py3-none-any.whl.metadata\n",
      "  Downloading ipython-8.16.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis)\n",
      "  Obtaining dependency information for jsonpickle>=1.4.1 from https://files.pythonhosted.org/packages/d3/25/6e0a450430b7aa194b0f515f64820fc619314faa289458b7dfca4a026114/jsonpickle-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading jsonpickle-3.0.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->llama_index) (3.20.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/homebrew/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama_index) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/homebrew/lib/python3.11/site-packages (from httplib2>=0.20.0->nebula3-python) (3.1.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
      "Collecting backcall (from ipython>=5.3.0->pyvis)\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting decorator (from ipython>=5.3.0->pyvis)\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
      "  Obtaining dependency information for jedi>=0.16 from https://files.pythonhosted.org/packages/20/9f/bc63f0f0737ad7a60800bfd472a4836661adae21f9c2535f3957b1e54ceb/jedi-0.19.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting matplotlib-inline (from ipython>=5.3.0->pyvis)\n",
      "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting pickleshare (from ipython>=5.3.0->pyvis)\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 (from ipython>=5.3.0->pyvis)\n",
      "  Obtaining dependency information for prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 from https://files.pythonhosted.org/packages/a9/b4/ba77c84edf499877317225d7b7bc047a81f7c2eed9628eeb6bab0ac2e6c9/prompt_toolkit-3.0.39-py3-none-any.whl.metadata\n",
      "  Downloading prompt_toolkit-3.0.39-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis) (2.16.1)\n",
      "Collecting stack-data (from ipython>=5.3.0->pyvis)\n",
      "  Obtaining dependency information for stack-data from https://files.pythonhosted.org/packages/f1/7b/ce1eafaf1a76852e2ec9b22edecf1daa58175c090266e9f6c64afcd81d91/stack_data-0.6.3-py3-none-any.whl.metadata\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting traitlets>=5 (from ipython>=5.3.0->pyvis)\n",
      "  Obtaining dependency information for traitlets>=5 from https://files.pythonhosted.org/packages/e0/ad/0ec97a5a37481552b43352190e509b8dfb2e379d55b41fac8ba5a635bb9a/traitlets-5.12.0-py3-none-any.whl.metadata\n",
      "  Downloading traitlets-5.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=5.3.0->pyvis)\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting appnope (from ipython>=5.3.0->pyvis)\n",
      "  Using cached appnope-0.1.3-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from Jinja2->ipython-ngql) (2.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/homebrew/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->llama_index) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision->sentence_transformers) (10.1.0)\n",
      "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=5.3.0->pyvis)\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=5.3.0->pyvis)\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting wcwidth (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis)\n",
      "  Obtaining dependency information for wcwidth from https://files.pythonhosted.org/packages/58/19/a9ce39f89cf58cf1e7ce01c8bb76ab7e2c7aadbc5a2136c3e192097344f5/wcwidth-0.2.8-py2.py3-none-any.whl.metadata\n",
      "  Downloading wcwidth-0.2.8-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting executing>=1.2.0 (from stack-data->ipython>=5.3.0->pyvis)\n",
      "  Obtaining dependency information for executing>=1.2.0 from https://files.pythonhosted.org/packages/bb/3f/748594706233e45fd0e6fb57a2fbfe572485009c52b19919d161a0ae5d52/executing-2.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading executing-2.0.0-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack-data->ipython>=5.3.0->pyvis)\n",
      "  Obtaining dependency information for asttokens>=2.1.0 from https://files.pythonhosted.org/packages/4f/25/adda9979586d9606300415c89ad0e4c5b53d72b92d2747a3c634701a6a02/asttokens-2.4.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading asttokens-2.4.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pure-eval (from stack-data->ipython>=5.3.0->pyvis)\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Using cached ipython_ngql-0.7.5-py3-none-any.whl (8.7 kB)\n",
      "Using cached ipython-8.16.1-py3-none-any.whl (806 kB)\n",
      "Using cached jsonpickle-3.0.2-py3-none-any.whl (40 kB)\n",
      "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached prompt_toolkit-3.0.39-py3-none-any.whl (385 kB)\n",
      "Using cached traitlets-5.12.0-py3-none-any.whl (84 kB)\n",
      "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached asttokens-2.4.0-py2.py3-none-any.whl (27 kB)\n",
      "Using cached executing-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached wcwidth-0.2.8-py2.py3-none-any.whl (31 kB)\n",
      "Installing collected packages: wcwidth, pure-eval, ptyprocess, pickleshare, executing, backcall, appnope, traitlets, prompt-toolkit, pexpect, parso, jsonpickle, decorator, asttokens, stack-data, matplotlib-inline, jedi, ipython-ngql, ipython, pyvis\n",
      "Successfully installed appnope-0.1.3 asttokens-2.4.0 backcall-0.2.0 decorator-5.1.1 executing-2.0.0 ipython-8.16.1 ipython-ngql-0.7.5 jedi-0.19.1 jsonpickle-3.0.2 matplotlib-inline-0.1.6 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.39 ptyprocess-0.7.0 pure-eval-0.2.2 pyvis-0.3.2 stack-data-0.6.3 traitlets-5.12.0 wcwidth-0.2.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install llama_index nebula3-python futures glob2 pypdf sentence_transformers langchain ipython-ngql pyvis networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a0f34f-c8df-4cc2-8a15-cc83e2272b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cb4bb3-8255-4d56-9da7-43bd6fbf7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"EDENAI_API_KEY\"] = \"*****.****.*****\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"******************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd803b7-8840-4bfd-9d37-f3395f6d1526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex, LLMPredictor, ServiceContext, utils\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.llms import EdenAI\n",
    "from IPython.display import Markdown, display\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a084564-e814-495d-ba71-e16a2b669e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./docs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a3b969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(id_='97119cf4-965e-4014-97f9-541a690d75f1', embedding=None, metadata={'page_label': '1', 'file_name': 'P02-1058.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='900803dc9e1b40eb30f903a63058fa2dcb46e1f9b2e20d75f46245da695c2dfe', text='From Single to Multi -document Summarization:  \\nA Prototype System and its Evaluation  \\nChin-Yew Lin and Eduard Hovy  \\nUniversity of Southern California / Information Sciences Institute  \\n4676 Admiralty Way  \\nMarina del Rey, CA 90292  \\n{cyl,hovy}@isi.edu  \\n \\nAbstract  \\nNeATS is a multi -document \\nsummarization system that attempts \\nto extract relevant or interesting \\nportions from a set of documents \\nabout some topic and present them \\nin coherent order. NeATS is among \\nthe best performers in the large scale \\nsummarization evaluat ion DUC \\n2001.  \\n1 Introduction  \\nIn recent years, text summarization has been \\nenjoying a period of revival.  Two workshops \\non Automatic Summarization were held in \\n2000 and 2001.  However, the area is still \\nbeing fleshed out: most past efforts have \\nfocused only o n single -document \\nsummarization (Mani 2000), and no standard \\ntest sets and large scale evaluations have been \\nreported or made available to the English -\\nspeaking research community except the \\nTIPSTER SUMMAC Text Summarization \\nevaluation (Mani et al. 1998).  \\nTo address these issues, the Document \\nUnderstanding Conference (DUC) sponsored \\nby the National Institute of Standards and \\nTechnology (NIST) started in 2001 in the \\nUnited States.  The Text Summarization \\nChallenge (TSC) task under the NTCIR (NII -\\nNACSIS Test C ollection for IR Systems) \\nproject started in 2000 in Japan.  DUC and \\nTSC both aim to compile standard training and \\ntest collections that can be shared among \\nresearchers and to provide common and large \\nscale evaluations in single and multiple \\ndocument summa rization for their participants.  \\nIn this paper we describe a multi -document \\nsummarization system NeATS.  It attempts to \\nextract relevant or interesting portions from a \\nset of documents about some topic and present them in coherent order.  We outline the \\nNeATS system and describe how it performs \\ncontent selection, filtering, and presentation in \\nSection 2.  Section 3 gives a brief overview of \\nthe evaluation procedure used in DUC -2001 \\n(DUC 2001).  Section 4 discusses evaluation \\nmetrics, and Section 5 the resul ts.  We \\nconclude with future directions.  \\n2 NeATS  \\nNeATS is an extraction -based multi -document \\nsummarization system.  It leverages techniques \\nproved effective in single document \\nsummarization such as: term frequency (Luhn \\n1969), sentence position (Lin and Hovy  1997), \\nstigma words (Edmundson 1969), and a \\nsimplified version of MMR (Goldstein et al. \\n1999) to select and filter content.  To improve \\ntopic coverage and readability, it uses term \\nclustering, a ‘buddy system’ of paired \\nsentences, and explicit time annota tion. \\nMost of the techniques adopted by NeATS are \\nnot new.  However, applying them in the \\nproper places to summarize multiple \\ndocuments and evaluating the results on large \\nscale common tasks are new.  \\nGiven an input of a collection of sets of \\nnewspaper arti cles, NeATS generates \\nsummaries in three stages: content selection, \\nfiltering, and presentation. We describe each \\nstage in the following sections.  \\n2.1 Content Selection  \\nThe goal of content selection is to identify \\nimportant concepts mentioned in a document \\ncollection.  For example, AA flight 11 , AA \\nflight 77 , UA flight 173 , UA flight 93 , New \\nYork, World Trade Center , Twin Towers , \\nOsama bin Laden , and al-Qaida  are key \\nconcepts for a document collection about the \\nSeptember 11 terrorist attacks in the US.                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 457-464.                         Proceedings of the 40th Annual Meeting of the Association for', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(documents))\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326ddab-8616-4e2b-978c-95ec9439bbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4148f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = \"--address 127.0.0.1 --port 9669 --user root --password nebula;\"\n",
    "space_name = \"xyz\"\n",
    "%load_ext ngql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69295598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Pool Created\n",
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xyz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name\n",
       "0  xyz"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql {connection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "491b3a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql CREATE SPACE IF NOT EXISTS {space_name}(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e90b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql USE {space_name};\n",
    "%ngql CREATE TAG IF NOT EXISTS entity(name string);\n",
    "%ngql CREATE EDGE IF NOT EXISTS relationship(relationship string);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f907fcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f57947e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhinavvengala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "902ee87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "llm = EdenAI(provider=\"openai\", model=\"text-davinci-003\", temperature=0, max_tokens=512)\n",
    "embeddings = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, chunk_size=512, embed_model=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "485fa702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NEBULA_USER\"] = \"root\"\n",
    "os.environ[\"NEBULA_PASSWORD\"] = \"nebula\"  # default password\n",
    "os.environ[\"NEBULA_ADDRESS\"] = (\n",
    "    \"127.0.0.1:9669\"  # assumed we have NebulaGraph installed locally\n",
    ")\n",
    "\n",
    "# space_name = \"xyz\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\n",
    "    \"relationship\"\n",
    "]  # default, could be omitted if created from an empty kg\n",
    "tags = [\"entity\"]\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f771d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NeATS, is, multi-document summarization system)\n",
      "(NeATS, attempts to extract, relevant or interesting portions)\n",
      "(NeATS, is among, best performers in large scale summarization evaluation DUC 2001)\n",
      "(DUC, sponsors, Text Summarization Challenge)\n",
      "(NTCIR, started in, 2000 in Japan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NeATS, is, multi-document summarization system)\n",
      "(NeATS, leverages, techniques)\n",
      "(NeATS, uses, term clustering)\n",
      "(NeATS, generates summaries in, three stages)\n",
      "(Content selection, goal is to, identify important concepts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(AA flight 11, is key concept for, September 11 terrorist attacks)\n",
      "(AA flight 77, is key concept for, September 11 terrorist attacks)\n",
      "(UA flight 173, is key concept for, September 11 terrorist attacks)\n",
      "(UA flight 93, is key concept for, September 11 terrorist attacks)\n",
      "(New York, is key concept for, September 11 terrorist attacks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NeATS, computes, likelihood ratio)\n",
      "(Dunning, 1993, introduced)\n",
      "(Lin and Hovy, 2000, introduced)\n",
      "(Clusters, formed through, lexical connection)\n",
      "(Ranking algorithm, rewards, specific concepts)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Sentence Position, used as, content filter)\n",
      "(Stigma Words, reduce scores of, sentences)\n",
      "(Maximum Marginal Relevancy, used as, content filter)\n",
      "(Edmundson, used, sentence position)\n",
      "(Marcu, used, sentence position)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Slovenia, has, federal army)\n",
      "(Yugoslavia, has, Slovenia Croatia)\n",
      "(Slovene, has, Milan Kucan)\n",
      "(Croatia, has, European Community)\n",
      "(Slovenian, has, foreign exchange)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(CMU's MMR, used to address, redundancy issue)\n",
      "(Content selection and filtering methods, concern, individual sentences)\n",
      "(Buddy system, introduced to improve, cohesion and coherence)\n",
      "(Definite noun phrases, addressed by, buddy system)\n",
      "(Events, spread along, extended timeline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(DUC-2001, used, first sentence)\n",
      "(multi-document summarization, has problem, time period)\n",
      "(time disambiguation, important in, multi-document summarization)\n",
      "(date expressions, used for, time disambiguation)\n",
      "(time disambiguation, compute, actual dates)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(US Drought of 1988, captured attention, Washington)\n",
      "(US Drought of 1988, pushed through, largest disaster relief measure)\n",
      "(US Drought of 1988, became, unexpected election-year windfall)\n",
      "(US Drought of 1988, hit, thousands of farmers)\n",
      "(US Drought of 1988, collected millions of dollars, nature's normal quirks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(DUC 2001, supported by, NIST)\n",
      "(DUC 2001, has tasks, fully automatic summarization)\n",
      "(DUC 2001, has tasks, exploratory summarization)\n",
      "(NIST, assessors created, ideal written summaries)\n",
      "(NIST, assessors compared, system-generated summaries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 27.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NIST, used, Summary Evaluation Environment)\n",
      "(Summary Evaluation Environment, developed by, Lin 2001)\n",
      "(Summary Evaluation Environment, provides interfaces for, assessors)\n",
      "(assessors, judge quality of, summaries)\n",
      "(summaries, evaluated at, five levels)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Slovenia, plans to begin work on, constitution)\n",
      "(Slovenia, give, full sovereignty)\n",
      "(Slovenia, raised, flag)\n",
      "(Yugoslavia, mobilized troops to regain control, Croatia)\n",
      "(Yugoslavia, normalization of relations, Slovenia)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Recall, measure, content retention)\n",
      "(Compression Ratio, defined as, length of summary/length of original document)\n",
      "(DUC-2001, set, compression lengths)\n",
      "(Overlap judgment, not binary, DUC-2001)\n",
      "(Assessor, judged, system units)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NIST assessors, marked, sharing relations)\n",
      "(McKeown et al., proposed, weighted recall)\n",
      "(weighted recall, treated, completeness of coverage)\n",
      "(system performance, compared, different threshold levels)\n",
      "(DUC-2001, proposed, different versions of weighted recall)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Recall t, defined as, follows)\n",
      "(Weighted retention, defined as, follows)\n",
      "(Unweighted retention, defined as, follows)\n",
      "(Precision, borrowed from, information retrieval research)\n",
      "(Precision, used to measure, how effectively a system generates good summary sentences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Retention 1, equal to, Precision metric)\n",
      "(Retention w, computed using, formulas)\n",
      "(NeATS, consistently ranked among, top 3)\n",
      "(NeATS, performance for, pseudo precision)\n",
      "(humans, score better than, any system)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 27.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NIST assessors, wrote, two separate summaries)\n",
      "(Table 1, includes, pseudo precision)\n",
      "(Table 1, includes, unweighted retention)\n",
      "(Table 1, includes, weighted retention)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(page_label, is, 6)\n",
      "(file_name, is, P02-1058.pdf)\n",
      "(87%, is, 47.16%)\n",
      "(48.96%, is, 35.53%)\n",
      "(N*, is, 58.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Philz, founded in, 1982)\n",
      "(page_label, is, 6)\n",
      "(file_name, is, P02-1058.pdf)\n",
      "(percentages, range from, 8.02%)\n",
      "(percentages, range to, 55.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(page_label, is, 6)\n",
      "(file_name, is, P02-1058.pdf)\n",
      "(93%, is, 28.31%)\n",
      "(27.01%, is, 15.46%)\n",
      "(U, is, 23.88%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NeATS, performed better on, longer summaries)\n",
      "(NeATS, is sentence extraction-based, nature)\n",
      "(System Y, was best in, shorter summaries)\n",
      "(Table 2, shows macro-averaged scores for, humans, baselines, and 12 systems)\n",
      "(Table 2, assigns score of, 4 to all, 3 to most, 2 to some, 1 to hardly any, and 0 to none)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 32.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Most systems, scored well in, grammaticality)\n",
      "(NeATS, did not fare badly in, quality measures)\n",
      "(Humans, scored, highest in coherence)\n",
      "(Lead baseline, did well in, cohesion)\n",
      "(Coverage baseline, did poorly in, cohesion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NeATS, fared, quality measures)\n",
      "(NeATS, employed strategies, stigma word filtering)\n",
      "(NeATS, employed strategies, adding lead sentence)\n",
      "(NeATS, employed strategies, time annotation)\n",
      "(Table 2, averaged, grammaticality, cohesion, coherence)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NeATS, uses, simple methods)\n",
      "(NeATS, reducing redundancy, MMR)\n",
      "(NeATS, presenting summary sentences, chronological order)\n",
      "(NeATS, improving capability, content selection)\n",
      "(NeATS, enhancing cohesion, discourse processing)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Goldstein et al., published, paper)\n",
      "(Lin & Hovy, presented, paper)\n",
      "(Lin, developed, Summary Evaluation Environment)\n",
      "(Luhn, published, paper)\n",
      "(Mani et al., published, paper)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 30.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(McKeown et al., presented at, NAACL-2001 Workshop)\n",
      "(McKeown et al., presented at, DUC-01 Workshop)\n",
      "(Radev & McKeown, published in, Computational Linguistics)\n",
      "(Radev & McKeown, published in, 1998)\n",
      "(Radev & McKeown, title of, Generating Natural Language Summaries from Multiple On-line Sources)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Multi-Document Summarization, builds on, single-document summarization methods)\n",
      "(Multi-Document Summarization, differs from, single-document summarization)\n",
      "(Multi-Document Summarization, addresses, compression, speed, redundancy and passage selection)\n",
      "(Multi-Document Summarization, uses, domain-independent techniques)\n",
      "(Multi-Document Summarization, uses, modular framework)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(IR, has not been integrated with, summarization)\n",
      "(summarization system, has greater functionality challenges, in true IR context)\n",
      "(user, issues, search query)\n",
      "(multi-document summarization, should contain, key shared relevant information)\n",
      "(multi-document summarization, has four significant differences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 29.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(group of topically-related articles, has, higher degree of redundancy)\n",
      "(temporal dimension, typical in, stream of news reports)\n",
      "(compression ratio, smaller for, collections of dozens or hundreds of topically related documents)\n",
      "(co-reference problem, presents greater challenges for, multi-document summarization)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(document summarization, requires, selection/evaluation/ordering/aggregation of items)\n",
      "(text-span deletion, attempts to delete, less important spans)\n",
      "(IBM, developed, automated document summarization)\n",
      "(discourse structure, investigated for, summarization)\n",
      "(machine learning, used to find, patterns in text)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Stein et al., 1999, includes, comparing templates)\n",
      "(TIPSTER, 1998b, finding, co-reference chains)\n",
      "(Mani and Bloedern, 1997, building, activation networks)\n",
      "(McKeown et al., 1999, creates, multi-document summary)\n",
      "(Columbia University system, uses, machine learning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 36.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Multi-Document Summarization, requires, two types of situations)\n",
      "(Multi-Document Summarization, requires, elimination of redundancy)\n",
      "(Multi-Document Summarization, requires, user information seeking goals)\n",
      "(Multi-Document Summarization, requires, interface for system)\n",
      "(Multi-Document Summarization, requires, relevance feedback mechanism)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(clustering, ability to, find related information)\n",
      "(coverage, ability to, find main points)\n",
      "(anti-redundancy, ability to, minimize redundancy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 32.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(summary cohesion criteria, include, document ordering)\n",
      "(summary cohesion criteria, include, news-story principle)\n",
      "(summary cohesion criteria, include, topic-cohesion)\n",
      "(summary cohesion criteria, include, time line ordering)\n",
      "(coherence, include, sufficient context)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 34.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Multi-Document Summarizers, have, 4 types)\n",
      "(Summary from Common Sections of Documents, finds, important relevant parts)\n",
      "(Centroid Document Summary, creates, single document summary)\n",
      "(Centroid Document plus Outliers Summary, creates, single document summary)\n",
      "(Latest Document plus Outliers Summary, creates, single document summary)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 35.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Common Sections, have in common, relevant parts)\n",
      "(Unique Sections, have, relevant parts)\n",
      "(Time Weighting Factor, weights, information)\n",
      "(Summary Extracts, involve, natural language processing)\n",
      "(Textwise, approach, multi-document summary)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 32.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(MMR-MD, is, metric)\n",
      "(MMR-MD, maximizes, marginal relevance)\n",
      "(MMR-MD, includes, cosine similarity metric)\n",
      "(MMR-MD, includes, coverage score)\n",
      "(MMR-MD, measures, relevance and novelty)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Sire2, uses, cosine similarity metric)\n",
      "(Sire2, penalizes, passages from clusters)\n",
      "(MMR-MD, computes, relevance-ranked list)\n",
      "(MMR-MD, computes, maximal diversity ranking)\n",
      "(MMR-MD, optimizes, linear combination)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(SMART, used to compute, cosine similarities)\n",
      "(MMR-MD, used as, summarization metric)\n",
      "(TIPSTER, provided, topical clusters)\n",
      "(apartheid-related news-wire documents, span, 1988-1992)\n",
      "(TIPSTER, provided, topic description)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(TIPSTER, provided, topic description)\n",
      "(200 documents, were on average, 31 sentences in length)\n",
      "(6115 sentences, used as, summary unit)\n",
      "(summary, generated, 10 sentences long)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 30.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Sire1, similarity metric for, relevance ranking)\n",
      "(Sim2, anti-redundancy metric for, documents)\n",
      "(R, subset of, passages)\n",
      "(R\\S, set difference of, passages)\n",
      "(timestamp, difference of, documents)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(page_label, is, 5)\n",
      "(file_name, is, W00-0405.pdf)\n",
      "(R, contains, unselected passages)\n",
      "(7vw, is subset of, clusters)\n",
      "(7~, is subset of, clusters)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(EW. de Klerk, proposed to repeal, apartheid)\n",
      "(Nelson Mandela, called on, international community)\n",
      "(Canadian anti-apartheid groups, urged, government)\n",
      "(South Africa, has, seven major military bases in Angola)\n",
      "(Angola, accused, South Africa of illegal occupation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(ANC, fighting to topple, South African government)\n",
      "(South African government, policy of, apartheid)\n",
      "(South African government, controls, economy)\n",
      "(SWAPO, fighting for, independence for Namibia)\n",
      "(Shultz, voiced concerns about, Soviet influence on ANC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(data sets, constructed for, experimental evaluations of multi-document summarization)\n",
      "(standard IR technique, insufficient for, multi-document summarization)\n",
      "(data sets, allow to measure, effects of features on multi-document summarization quality)\n",
      "(data sets, contain, snapshot of an event from multiple sources)\n",
      "(data sets, contain, unfoldment of an event over time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(George Shultz, voiced concerns about, Soviet influence)\n",
      "(African National Congress, use of violence in, struggle against apartheid)\n",
      "(South Africa, wants closed down, ANC military bases)\n",
      "(Pope, should have spoken out more forcefully against, white-minority government's policies of apartheid)\n",
      "(Harare summit, conditions included, removal of all troops from South Africa's black townships)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(African National Congress, suspended, armed struggle)\n",
      "(African National Congress, forged, sanctions strategy)\n",
      "(African National Congress, took, tough line)\n",
      "(President de Klerk, proposed, repeal apartheid)\n",
      "(Nelson Mandela, called on, international community)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(summarizer #6, compared to, summarizer #1)\n",
      "(summarizer #6, compared to, summarizer #3)\n",
      "(proposed summarizer, builds upon, single-document summarization)\n",
      "(approach, is domain-independent, statistical processing)\n",
      "(approach, maximizes, novelty of information)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Summarization system, based on, sophisticated natural language understanding)\n",
      "(Summarization system, lacks, co-reference resolution)\n",
      "(Future work, integrate, multi-document summarization with clustering)\n",
      "(Future work, investigate, generating coherent temporally based event summaries)\n",
      "(Future work, investigate, interactive interfaces to browse and explore large document sets)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Lexical chains, used for, text summarization)\n",
      "(Boguraev, Chris Kennedy, authored, Salience based content characterization)\n",
      "(Buckley, authored, SMART information retrieval system)\n",
      "(Carbonell, Goldstein, used, MMR reranking)\n",
      "(Goldstein, Kantrowitz, Mittal, Carbonell, authored, Summarizing Text Documents)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Lexical semantics, used in, summarization)\n",
      "(Julian M. Kupiec, Jan Pedersen, Francine Chen, authored, A trainable document summarizer)\n",
      "(P. H. Luhn, authored, Automatic creation of literature abstracts)\n",
      "(Inderjeet Mani, Eric Bloedern, authored, Multi-document summarization by graph search and merging)\n",
      "(Inderjeet Mani, Eric Bloedom, authored, Summarizing similarities and differences among related documents)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 36.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Mandar Mitra, Amit Singhal, Chris Buckley), authored, ACL/EACL-97 Workshop paper)\n",
      "(Chris D. Paice, authored, Info. Proc. and Management paper)\n",
      "(Dragomir Radev, Kathy McKeown, authored, Computational Linguistics paper)\n",
      "(Gerald Salton, authored, Journal of American Society for Information Sciences paper)\n",
      "(Gerald Salton, authored, Automatic Text Processing book)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 33.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(James Shaw, published, Conciseness through aggregation in text generation)\n",
      "(Gees C. Stein, Tomek Strzalkowski, G. Bowden Wise, published, Summarizing Multiple Documents Using Text Extraction and Interactive Clustering)\n",
      "(Tomek Strzalkowski, Jin Wang, Bowden Wise, published, A robust practical text summarization system)\n",
      "(J. I. Tait, published, Automatic Summarizing of English Texts)\n",
      "(Simone Teufel, Marc Moens, published, Sentence extraction as a classification task)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 42.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(page_label, is, 9)\n",
      "(file_name, is, W00-0405.pdf)\n",
      "(SIGIR'98, held in, 1998)\n",
      "(Yiming Yang, et al., published, Learning approaches for topic detection and tracking news events)\n",
      "(IEEE Intelligent Systems, published, Special Issue on Applications of Intelligent Information Retrieval)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 27.42it/s]\n"
     ]
    }
   ],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=5,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    include_embeddings=True,\n",
    "    tags=tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe579894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;32mExtracted keywords: ['multi', 'summarization', 'document', 'problems']\n",
      "\u001b[0mINFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: f7f5d96b-d7f9-4a57-9b7b-b058ecf6f42b: 1995. Lexical se- \n",
      "mantics in summarization. In Proceedings of the First \n",
      "Ann...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 797819d3-2f94-4924-8593-a9cc77aceb69: document summarization by using additional, available \n",
      "information about the ...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 3095c736-a667-4250-a0fa-34d033c9e339: i However, large- \n",
      "scale IR and summarization have not yet been truly in- \n",
      "te...\n",
      "\u001b[1;3;34mKG context:\n",
      "The following are knowledge sequence in max depth 2 in the form of directed graph like:\n",
      "`subject -[predicate]->, object, <-[predicate_next_hop]-, object_next_hop ...`\n",
      "summarization{name: summarization} <-[relationship:{relationship: has not been integrated with}]- IR{name: IR}\n",
      "summarization{name: summarization} <-[relationship:{relationship: used in}]- Lexical semantics{name: Lexical semantics}\n",
      "summarization{name: summarization} <-[relationship:{relationship: investigated for}]- discourse structure{name: discourse structure}\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Multi-document summarization has several problems, such as selecting, evaluating, ordering, and aggregating items of information according to their relevance to a particular subject or purpose. Additionally, the system must take into account other summaries that have already been generated, and must be able to summarize either complete document sets or single documents in the context of previously summarized ones.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kg_index_query_engine = kg_index.as_query_engine(\n",
    "    retriever_mode=\"keyword\",\n",
    "    verbose=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "# Using KG as a Query Engine (QA)\n",
    "response = kg_index_query_engine.query(\"problems in multi document summarization\")\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cdb07e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.retrievers import KGTableRetriever\n",
    "\n",
    "# Using KG as an Information Retriever\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"hybrid\", include_text=True\n",
    ")\n",
    "# retriever_mode\n",
    "# keyword: Uses keywords to fetch\n",
    "# embedding: Uses embeddings\n",
    "# hybrid: Uses both\n",
    "\n",
    "# include_text=True  (return the nodes as text without unnecessary characters)\n",
    "# inlcude_text=False (returns the nodes as nodes and pointers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f5e4bbc9-88a0-4e99-b336-c286282ec214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 132dce5a-b0ac-49a8-995a-4137e17dc628: If we count the number of model units that are \n",
      "marked as good summary units ...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 5bb8f509-52e6-4fba-ac18-dda60bc94c7f: In a k ey step for locating important sentences, \n",
      "NeATS computes the likeliho...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 43710785-afef-4cfd-88e3-d365924a63e6: We described a multi -document \n",
      "summarization system, NeATS, and its \n",
      "evaluat...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 522dbc1d-6914-4cd9-9439-935c1fb5cbb3: In this paper we describe a multi -document \n",
      "summarization system NeATS.  It ...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: b3f70797-b8ab-40b0-aae3-9345f2bd9639: From Single to Multi -document Summarization:  \n",
      "A Prototype System and its Ev...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: c69006cb-31a6-4850-ac55-2f88d6abb8ff: With this in \n",
      "mind, we have the following observations.  \n",
      "(1) Most systems sc...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 9fb31a6f-2b98-4a9f-9f74-4b974c6d4853: the best, followed by N, and then T; if we \n",
      "choose averaged weighted retentio...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: f4c7db9c-e312-42a4-b616-f798dfd42967: (4) NeATS did not fare badly in quality \n",
      "measures.  It was in the same catego...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nodes = kg_retriever.retrieve(\"NeATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bba751f2-bd8a-4be8-9048-d2bb9aaf65d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If we count the number of model units that are \\nmarked as good summary units and are \\nselected by systems, and use the number of \\nmodel units in various summary lengths as the \\nsample space, we obta in a precision metric \\nequal to Retention 1.  Alternatively, we can \\ncount how many unique system units share \\ncontent with model units and use the total \\nnumber of system units as the sample space.  \\nWe define this as pseudo precision, Precision p, \\nas follows:  \\nsummary system in the SUs of number  Totalmarked SUs of Number  \\nMost of the participants in DUC -2001 reported \\ntheir pseudo precision figures.  \\n5 Results and Discussion  \\nWe present the performance of NeATS in \\nDUC -2001 in content and quality measures.  \\n5.1 Content  \\nWith respect to content, we computed \\nRetention 1, Retention w, and Precision p using \\nthe formulas defined in the previous section.  \\nThe scores are shown in Table 1 (overall \\naverage and per size).  Analyzing all systems’ \\nresults according to these, we made the \\nfollowing observations.  \\n (1) NeATS (syst em N) is consistently ranked \\namong the top 3 in average and per size \\nRetention 1 and Retention w. \\n(2) NeATS’s performance for averaged pseudo \\nprecision equals human’s at about 58% (P p all). (3) The performance in weighted retention is \\nreally low.  Even human s6 score only 29% (R w \\nall). This indicates low inter -human agreement \\n(which we take to reflect the undefinedness of \\nthe ‘generic summary’ task).  However, the \\nunweighted retention of humans is 53%.  This \\nsuggests assessors did write something similar \\nin th eir summaries but not exactly the same; \\nonce again illustrating the difficulty of \\nsummarization evaluation.  \\n(4) Despite the low inter -human agreement, \\nhumans score better than any system.In a k ey step for locating important sentences, \\nNeATS computes the likelihood ratio λ \\n(Dunning, 1993) to identify key concepts in \\nunigrams, bigrams, and trigrams 1, using the \\non- topic document collection as the relevant \\nset and the off -topic document collection as the \\nirrelevant set.  Figure 1 shows the top 5 \\nconcepts with their relevancy scores ( -2λ) for \\nthe topic “ Slovenia Secession from \\nYugoslavia ” in the DUC -2001 test collection.  \\nThis is similar to the idea of topic signature \\nintroduced in (Lin and Hovy 2000 ). \\nWith the individual key concepts available, we \\nproceed to cluster these concepts in order to \\nidentify major subtopics within the main topic. \\nClusters are formed through strict lexical \\nconnection.  For example, Milan  and Kucan  \\nare grouped as “ Milan Kucan ” since “ Milan \\nKucan ” is a key bigram concept; while \\nCroatia, Yugoslavia , Slovenia , republic , and \\nare joined due to the connections as follows:  \\n• Slovenia Croatia  \\n• Croatia Slovenia  \\n• Yugoslavia Slovenia  \\n• republic Slovenia  \\n                                                 \\n1 Closed class words ( of, in, and, are , and so on) \\nwere ignored in constructing unigrams, bigrams and \\ntrigrams.  • Croatia republic  \\nEach sentence in the document set is  then \\nranked, using the key concept structures. An \\nexample is shown in Figure 2.  The ranking \\nalgorithm rewards most specific concepts first; \\nfor example, a sentence containing “ Milan \\nKucan ” has a higher score than a sentence \\ncontains only e ither Milan  or Kucan .We described a multi -document \\nsummarization system, NeATS, and its \\nevaluation in DUC -2001. We were encouraged \\nby the content and readability of the results.  \\nAs a prototype system, NeATS deliberately \\nused simple methods guided by a few \\nprinciples:  \\n• Extracting important concepts based on \\nreliable statistics.  \\n• Filtering sentences by their positions and \\nstigma words.  \\n• Reducing redundancy using MMR.  \\n• Presenting summary sentences in their \\nchronological order with time annotations.  \\nThese simple principles worked effecti vely.  \\nHowever, the simplicity of the system also \\nlends itself to further improvements.  We \\nwould like to apply some compression \\ntechniques or use linguistic units smaller than \\nsentences to improve our retention score.  The \\nfact that NeATS performed as wel l as the \\nhuman in pseudo precision but did less well in \\nretention indicates its summaries might include \\ngood but duplicated information.  Working \\nwith sub -sentence units should help.  \\nTo improve NeATS’s capability in content \\nselection, we have started to p arse sentences \\ncontaining key unigram, bigram, and trigram \\nconcepts to identify their relations within their \\nconcept clusters.  \\nTo enhance cohesion and coherence, we are \\nlooking into incorporating discourse \\nprocessing techniques (Marcu 1999) or Radev \\nand Mc Keown’s (1998) summary operators.  \\nWe are analyzing the DUC evaluation scores \\nin the hope of suggesting improved and more \\nstable metrics.  \\nReferences  \\nDUC. 2001. The Document Understanding \\nWorkshop 2001. http://www -nlpir.nist.gov/ \\nprojects/duc/2001.html.  \\nDunn ing, T. 1993. Accurate Methods for the \\nStatistics of Surprise and Coincidence.  \\nComputational Linguistics  19, 61 –74. \\nEdmundson, H.P. 1969. New Methods in \\nAutomatic Abstracting.In this paper we describe a multi -document \\nsummarization system NeATS.  It attempts to \\nextract relevant or interesting portions from a \\nset of documents about some topic and present them in coherent order.  We outline the \\nNeATS system and describe how it performs \\ncontent selection, filtering, and presentation in \\nSection 2.  Section 3 gives a brief overview of \\nthe evaluation procedure used in DUC -2001 \\n(DUC 2001).  Section 4 discusses evaluation \\nmetrics, and Section 5 the resul ts.  We \\nconclude with future directions.  \\n2 NeATS  \\nNeATS is an extraction -based multi -document \\nsummarization system.  It leverages techniques \\nproved effective in single document \\nsummarization such as: term frequency (Luhn \\n1969), sentence position (Lin and Hovy  1997), \\nstigma words (Edmundson 1969), and a \\nsimplified version of MMR (Goldstein et al. \\n1999) to select and filter content.  To improve \\ntopic coverage and readability, it uses term \\nclustering, a ‘buddy system’ of paired \\nsentences, and explicit time annota tion. \\nMost of the techniques adopted by NeATS are \\nnot new.  However, applying them in the \\nproper places to summarize multiple \\ndocuments and evaluating the results on large \\nscale common tasks are new.  \\nGiven an input of a collection of sets of \\nnewspaper arti cles, NeATS generates \\nsummaries in three stages: content selection, \\nfiltering, and presentation. We describe each \\nstage in the following sections.  \\n2.1 Content Selection  \\nThe goal of content selection is to identify \\nimportant concepts mentioned in a document \\ncollection.From Single to Multi -document Summarization:  \\nA Prototype System and its Evaluation  \\nChin-Yew Lin and Eduard Hovy  \\nUniversity of Southern California / Information Sciences Institute  \\n4676 Admiralty Way  \\nMarina del Rey, CA 90292  \\n{cyl,hovy}@isi.edu  \\n \\nAbstract  \\nNeATS is a multi -document \\nsummarization system that attempts \\nto extract relevant or interesting \\nportions from a set of documents \\nabout some topic and present them \\nin coherent order. NeATS is among \\nthe best performers in the large scale \\nsummarization evaluat ion DUC \\n2001.  \\n1 Introduction  \\nIn recent years, text summarization has been \\nenjoying a period of revival.  Two workshops \\non Automatic Summarization were held in \\n2000 and 2001.  However, the area is still \\nbeing fleshed out: most past efforts have \\nfocused only o n single -document \\nsummarization (Mani 2000), and no standard \\ntest sets and large scale evaluations have been \\nreported or made available to the English -\\nspeaking research community except the \\nTIPSTER SUMMAC Text Summarization \\nevaluation (Mani et al. 1998).  \\nTo address these issues, the Document \\nUnderstanding Conference (DUC) sponsored \\nby the National Institute of Standards and \\nTechnology (NIST) started in 2001 in the \\nUnited States.  The Text Summarization \\nChallenge (TSC) task under the NTCIR (NII -\\nNACSIS Test C ollection for IR Systems) \\nproject started in 2000 in Japan.  DUC and \\nTSC both aim to compile standard training and \\ntest collections that can be shared among \\nresearchers and to provide common and large \\nscale evaluations in single and multiple \\ndocument summa rization for their participants.  \\nIn this paper we describe a multi -document \\nsummarization system NeATS.With this in \\nmind, we have the following observations.  \\n(1) Most systems scored well in \\ngrammaticality.  This is not a surpris e since \\nmost of the participants extracted sentences as \\nsummaries.  \\nBut no system or human scored perfect in \\ngrammaticality. This might be due to the \\nartifact of cutting sentences at the 50, 100, 200, \\nand 400 words boundaries.  Only system Y \\nscored lower t han 3, which reflects its headline \\ninclusion strategy.  \\n(2) When it came to the measure for cohesion \\nthe results are confusing.  If even the human -\\nmade summaries score only 2.74 out of 4, it is \\nunclear what this category means, or how the \\nassessors arrived at these scores.  However, the \\nhumans and baseline 1 (lead baseline) did \\nscore in the upper range of 2 to 3 and all others \\nhad scores lower than 2.5.  Some of the \\nsystems (including B2) fell into the range of 1 \\nto 2 meaning some or hardly any cohesion.  \\nThe lead baseline (B1), taking the first 50, 100, \\n200, 400 words from the last document of a \\ntopic, did well.  On the contrary, the coverage \\nbaseline (B2) did poorly.  This indicates the \\ndifficulty of fitting sentences from different \\ndocuments together.  Even  selecting \\ncontinuous sentences from the same document \\n(B1) seems not to work well.  We need to \\ndefine this metric more clearly and improve \\nthe capabilities of systems in this respect.  \\n(3) Coherence scores roughly track cohesion \\nscores.  Most systems did b etter in coherence \\nthan in cohesion.   The human is the only one \\nscoring above 3.  Again the room for \\nimprovement is abundant.  \\n(4) NeATS did not fare badly in quality \\nmeasures.the best, followed by N, and then T; if we \\nchoose averaged weighted retention (R w all ), T \\nis the best, followed by N, and then Y.  The \\nreversal of T and Y due to different metrics \\ndemonstrates the importance of common \\nagreed upon metrics.  We believe  that metrics \\nhave to take coverage score ( C, Section 4.1.1) \\ninto consideration to be reasonable since most \\nof the content sharing among system units and \\nmodel units is partial.  The recall at threshold t, \\nRecall t (Section 4.1.1), proposed by \\n(McKeown et a l. 2001), is a good example.  In \\ntheir evaluation, NeATS ranked second at t=1, \\n3, 4 and first at t=2.   \\n(7) According to Table 1, NeATS performed \\nbetter on longer summaries (400 and 200 \\nwords) based on weighted retention than it did \\non shorter ones.  This is the result of the \\nsentence extraction -based nature of NeATS.  \\nWe expect that systems that use syntax -based \\nalgorithms to compress their output will \\nthereby gain more space to include additional \\nimportant material. For example, System Y \\nwas the best in s horter summaries.  Its 100 - \\nand 50 -word summaries contain only \\nimportant headlines.  The results confirm this \\nis a very effective strategy in composing short \\nsummaries.  However, the quality of the \\nsummaries suffered because of the \\nunconventional syntactic  structure of news \\nheadlines (Table 2).  \\n5.2 Quality  \\nTable 2 shows the macro -averaged scores for \\nthe humans, two baselines, and 12 systems.  \\nWe assign a score of 4 to all, 3 to most, 2 to \\nsome , 1 to hardly any , and 0 to none.  The \\nvalue assignment is for conv enience of computing averages, since it is more \\nappropriate to treat these measures as stepped \\nvalues instead of continuous ones.(4) NeATS did not fare badly in quality \\nmeasures.  It was in the same categories as \\nother top performers: grammaticality is \\nbetwe en most  and all, cohesion, some  and \\nmost, and coherence, some and most.  This \\nindicates the strategies employed by NeATS \\n(stigma word filtering, adding lead sentence, \\nand time annotation) worked to some extent \\nbut left room for improvement.  \\n6 Conclusions  Table 2. Averaged grammaticality, cohesion, and \\ncoherence over all summary sizes.  SYS Grammar Cohesion Coherence\\nHuman 3.74 2.74 3.19\\nB1 3.18 2.63 2.8\\nB2 3.26 1.71 1.65\\nL 3.72 1.83 1.9\\nM 3.54 2.18 2.4\\nN*3.65 2 2.22\\nO 3.78 2.15 2.33\\nP 3.67 1.93 2.17\\nR 3.6 2.16 2.45\\nS 3.67 1.93 2.04\\nT 3.51 2.34 2.61\\nU 3.28 1.31 1.11\\nW 3.13 1.48 1.28\\nY 2.45 1.73 1.77\\nZ 3.28 1.8 1.94'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(nodes))\n",
    "text = \"\"\n",
    "for i in nodes[:-1]:\n",
    "    text += i.text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f23353a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "Write brief summary of {topic} by fully utilizing information:\n",
    "```{nodes}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\", \"nodes\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "summary = llm_chain({\"topic\": \"NeATS\", \"nodes\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5410b078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order. It leverages techniques such as term frequency, sentence position, stigma words, and a simplified version of MMR to select and filter content. It also uses term clustering, a ‘buddy system’ of paired sentences, and explicit time annotation to improve topic coverage and readability. NeATS was evaluated in the Document Understanding Conference (DUC) 2001 and Text Summarization Challenge (TSC) 2000, and performed well in content and readability measures. It was consistently ranked among the top 3 in average and per size Retention 1 and Retention w, and its performance for averaged pseudo precision equals human’s at about 58%. However, the unweighted retention of humans is 53%, suggesting assessors did write something similar in their summaries but not exactly the same. NeATS is still a prototype system and can be improved with compression techniques, linguistic units smaller than sentences, discourse processing techniques, and improved and more stable metrics.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{summary['text']}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c587416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4e500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51970a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe23ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
