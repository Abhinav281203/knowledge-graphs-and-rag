{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db513d5d-44d8-4d3c-a856-de5dee38645c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.8.48)\n",
      "Requirement already satisfied: nebula3-python in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.4.0)\n",
      "Requirement already satisfied: futures in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: glob2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.7)\n",
      "Requirement already satisfied: pypdf in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.16.4)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.4.49 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (2.0.22)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (0.5.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (2023.10.0)\n",
      "Requirement already satisfied: langchain>=0.0.303 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (0.0.320)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (1.26.1)\n",
      "Requirement already satisfied: openai>=0.26.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (0.28.1)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (2.1.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama_index) (1.26.18)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nebula3-python) (0.22.0)\n",
      "Requirement already satisfied: future>=0.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nebula3-python) (0.18.3)\n",
      "Requirement already satisfied: six>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nebula3-python) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nebula3-python) (2023.3.post1)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (4.66.1)\n",
      "Collecting torch>=1.6.0 (from sentence_transformers)\n",
      "  Downloading torch-2.1.0-cp311-none-macosx_11_0_arm64.whl.metadata (24 kB)\n",
      "Collecting torchvision (from sentence_transformers)\n",
      "  Downloading torchvision-0.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.11.3-cp311-cp311-macosx_12_0_arm64.whl.metadata (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.4/165.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece (from sentence_transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->llama_index) (3.20.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httplib2>=0.20.0->nebula3-python) (3.1.1)\n",
      "Collecting filelock (from huggingface-hub>=0.4.0->sentence_transformers)\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.303->llama_index) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.303->llama_index) (3.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.303->llama_index) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.303->llama_index) (0.0.49)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.303->llama_index) (2.4.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.10.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.0)\n",
      "Collecting sympy (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (3.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->llama_index) (2023.3)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->sentence_transformers)\n",
      "  Downloading Pillow-10.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain>=0.0.303->llama_index) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain>=0.0.303->llama_index) (1.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama_index) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain>=0.0.303->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain>=0.0.303->llama_index) (2.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Collecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->sentence_transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.0-cp311-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.3-cp311-cp311-macosx_12_0_arm64.whl (29.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.0-cp311-cp311-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading Pillow-10.1.0-cp311-cp311-macosx_11_0_arm64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp311-cp311-macosx_11_0_arm64.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.4/425.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=2f32340464653c069c2cf14e0bb0e466b81d83209fbf2b27569c63d49c8613df\n",
      "  Stored in directory: /Users/abhinavvengala/Library/Caches/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentencepiece, mpmath, threadpoolctl, sympy, scipy, safetensors, pillow, filelock, torch, scikit-learn, huggingface-hub, torchvision, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed filelock-3.12.4 huggingface-hub-0.17.3 mpmath-1.3.0 pillow-10.1.0 safetensors-0.4.0 scikit-learn-1.3.2 scipy-1.11.3 sentence_transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.2.0 tokenizers-0.14.1 torch-2.1.0 torchvision-0.16.0 transformers-4.34.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install llama_index nebula3-python futures glob2 pypdf sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483a9d2-daa8-4e91-9418-3d848fd9666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06135785-e94d-4ae2-8ff2-ab5a84b45089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"EDENAI_API_KEY\"] = \"*****.****.*****\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"******************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c2953bc-30ed-46c5-aca4-69fd17cc7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex, LLMPredictor, ServiceContext\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.llms import EdenAI\n",
    "from IPython.display import Markdown, display\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a4e07-be96-4e9b-b5d9-5031c488e441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc66e6c4-4720-4081-93ba-f25e7fdfd102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ngql extension is already loaded. To reload it, use:\n",
      "  %reload_ext ngql\n"
     ]
    }
   ],
   "source": [
    "connection = \"--address 127.0.0.1 --port 9669 --user root --password nebula;\"\n",
    "space_name = \"kap\"\n",
    "%load_ext ngql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c450ed-359e-4f9b-87c7-d4ca67e05716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Pool Created\n",
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql {connection}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d233d2a-289d-45fd-86b3-5cabed2c06a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql CREATE SPACE IF NOT EXISTS {space_name}(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c1494c-9e53-4afb-8eaa-a507f6d90657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "USE kap;\n",
    "CREATE TAG IF NOT EXISTS entity(name string);\n",
    "CREATE EDGE IF NOT EXISTS relationship(relationship string);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe3b85e-5625-4420-9b1d-c8f9efe3ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c7a0d-3c41-412d-a5d6-eae4b3d5405e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3d7a495-f2b5-4d0e-b72a-6267cc8007b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = EdenAI(\n",
    "    provider=\"openai\", model=\"text-davinci-003\", temperature=0, max_tokens=1024\n",
    ")\n",
    "# embed_llm = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor, chunk_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c33c0ef-cf69-4a87-98de-1663b7bd25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NEBULA_USER\"] = \"root\"\n",
    "os.environ[\"NEBULA_PASSWORD\"] = \"nebula\"  # default password\n",
    "os.environ[\"NEBULA_ADDRESS\"] = (\n",
    "    \"127.0.0.1:9669\"  # assumed we have NebulaGraph installed locally\n",
    ")\n",
    "\n",
    "space_name = \"kap\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\n",
    "    \"relationship\"\n",
    "]  # default, could be omitted if created from an empty kg\n",
    "tags = [\"entity\"]\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b5108b7-56fb-4d29-877e-8d4410d89a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import download_loader\n",
    "\n",
    "# WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "# loader = WikipediaReader()\n",
    "\n",
    "# documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b7ebfd2-8c35-41f3-93d1-19d90ed72220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./docs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8e28527-ffd2-4e92-afbd-1756737a27f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e52816af-9a48-4693-94a0-63936efe35ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='b707b9a8-e54f-4ebd-a6ff-5a654f4876db', embedding=None, metadata={'page_label': '1', 'file_name': 'P02-1058.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='900803dc9e1b40eb30f903a63058fa2dcb46e1f9b2e20d75f46245da695c2dfe', text='From Single to Multi -document Summarization:  \\nA Prototype System and its Evaluation  \\nChin-Yew Lin and Eduard Hovy  \\nUniversity of Southern California / Information Sciences Institute  \\n4676 Admiralty Way  \\nMarina del Rey, CA 90292  \\n{cyl,hovy}@isi.edu  \\n \\nAbstract  \\nNeATS is a multi -document \\nsummarization system that attempts \\nto extract relevant or interesting \\nportions from a set of documents \\nabout some topic and present them \\nin coherent order. NeATS is among \\nthe best performers in the large scale \\nsummarization evaluat ion DUC \\n2001.  \\n1 Introduction  \\nIn recent years, text summarization has been \\nenjoying a period of revival.  Two workshops \\non Automatic Summarization were held in \\n2000 and 2001.  However, the area is still \\nbeing fleshed out: most past efforts have \\nfocused only o n single -document \\nsummarization (Mani 2000), and no standard \\ntest sets and large scale evaluations have been \\nreported or made available to the English -\\nspeaking research community except the \\nTIPSTER SUMMAC Text Summarization \\nevaluation (Mani et al. 1998).  \\nTo address these issues, the Document \\nUnderstanding Conference (DUC) sponsored \\nby the National Institute of Standards and \\nTechnology (NIST) started in 2001 in the \\nUnited States.  The Text Summarization \\nChallenge (TSC) task under the NTCIR (NII -\\nNACSIS Test C ollection for IR Systems) \\nproject started in 2000 in Japan.  DUC and \\nTSC both aim to compile standard training and \\ntest collections that can be shared among \\nresearchers and to provide common and large \\nscale evaluations in single and multiple \\ndocument summa rization for their participants.  \\nIn this paper we describe a multi -document \\nsummarization system NeATS.  It attempts to \\nextract relevant or interesting portions from a \\nset of documents about some topic and present them in coherent order.  We outline the \\nNeATS system and describe how it performs \\ncontent selection, filtering, and presentation in \\nSection 2.  Section 3 gives a brief overview of \\nthe evaluation procedure used in DUC -2001 \\n(DUC 2001).  Section 4 discusses evaluation \\nmetrics, and Section 5 the resul ts.  We \\nconclude with future directions.  \\n2 NeATS  \\nNeATS is an extraction -based multi -document \\nsummarization system.  It leverages techniques \\nproved effective in single document \\nsummarization such as: term frequency (Luhn \\n1969), sentence position (Lin and Hovy  1997), \\nstigma words (Edmundson 1969), and a \\nsimplified version of MMR (Goldstein et al. \\n1999) to select and filter content.  To improve \\ntopic coverage and readability, it uses term \\nclustering, a ‘buddy system’ of paired \\nsentences, and explicit time annota tion. \\nMost of the techniques adopted by NeATS are \\nnot new.  However, applying them in the \\nproper places to summarize multiple \\ndocuments and evaluating the results on large \\nscale common tasks are new.  \\nGiven an input of a collection of sets of \\nnewspaper arti cles, NeATS generates \\nsummaries in three stages: content selection, \\nfiltering, and presentation. We describe each \\nstage in the following sections.  \\n2.1 Content Selection  \\nThe goal of content selection is to identify \\nimportant concepts mentioned in a document \\ncollection.  For example, AA flight 11 , AA \\nflight 77 , UA flight 173 , UA flight 93 , New \\nYork, World Trade Center , Twin Towers , \\nOsama bin Laden , and al-Qaida  are key \\nconcepts for a document collection about the \\nSeptember 11 terrorist attacks in the US.                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 457-464.                         Proceedings of the 40th Annual Meeting of the Association for', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9cb78f68-c282-46c6-a33b-d6c8940d68b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(NeATS, is, multi-document summarization system)\n",
      "(NeATS, attempts to extract, relevant or interesting portions)\n",
      "(NeATS, is among, best performers in large scale summarization evaluation DUC 2001)\n",
      "(DUC, sponsors, Text Summarization Challenge)\n",
      "(NTCIR, started in, 2000 in Japan)\n",
      "\n",
      "(NeATS, is, multi-document summarization system)\n",
      "(NeATS, leverages, techniques)\n",
      "(NeATS, uses, term clustering)\n",
      "(NeATS, generates summaries in, three stages)\n",
      "(Content selection, goal is to, identify important concepts)\n",
      "\n",
      "(AA flight 11, is key concept for, September 11 terrorist attacks)\n",
      "(AA flight 77, is key concept for, September 11 terrorist attacks)\n",
      "(UA flight 173, is key concept for, September 11 terrorist attacks)\n",
      "(UA flight 93, is key concept for, September 11 terrorist attacks)\n",
      "(New York, is key concept for, September 11 terrorist attacks)\n",
      "\n",
      "(NeATS, computes, likelihood ratio)\n",
      "(Dunning, 1993, introduced)\n",
      "(Lin and Hovy, 2000, introduced)\n",
      "(Clusters, formed through, lexical connection)\n",
      "(Ranking algorithm, rewards, specific concepts)\n",
      "\n",
      "(Sentence Position, used as, content filter)\n",
      "(Stigma Words, reduce scores of, sentences)\n",
      "(Maximum Marginal Relevancy, used as, content filter)\n",
      "(Edmundson, used, sentence position)\n",
      "(Marcu, used, sentence position)\n",
      "\n",
      "(Slovenia, has, federal army)\n",
      "(Yugoslavia, has, Slovenia Croatia)\n",
      "(Slovene, has, Milan Kucan)\n",
      "(Croatia, has, European Community)\n",
      "(Slovenian, has, foreign exchange)\n",
      "\n",
      "(CMU's MMR, used to address, redundancy issue)\n",
      "(Content selection and filtering methods, concern, individual sentences)\n",
      "(Buddy system, introduced to improve, cohesion and coherence)\n",
      "(Definite noun phrases, addressed by, buddy system)\n",
      "(Events, spread along, extended timeline)\n",
      "\n",
      "(DUC-2001, used, first sentence)\n",
      "(multi-document summarization, has problem, time period)\n",
      "(time disambiguation, important in, multi-document summarization)\n",
      "(date expressions, used for, time disambiguation)\n",
      "(time disambiguation, compute, actual dates)\n",
      "\n",
      "(US Drought of 1988, captured attention, Washington)\n",
      "(US Drought of 1988, pushed through, largest disaster relief measure)\n",
      "(US Drought of 1988, became, unexpected election-year windfall)\n",
      "(US Drought of 1988, hit, thousands of farmers)\n",
      "(US Drought of 1988, collected millions of dollars, nature's normal quirks)\n",
      "\n",
      "(DUC 2001, supported by, NIST)\n",
      "(DUC 2001, has tasks, fully automatic summarization)\n",
      "(DUC 2001, has tasks, exploratory summarization)\n",
      "(NIST, assessors created, ideal written summaries)\n",
      "(NIST, assessors compared, system-generated summaries)\n",
      "\n",
      "(NIST, used, Summary Evaluation Environment)\n",
      "(Summary Evaluation Environment, developed by, Lin 2001)\n",
      "(Summary Evaluation Environment, provides interfaces for, assessors)\n",
      "(assessors, judge quality of, summaries)\n",
      "(summaries, evaluated at, five levels)\n",
      "\n",
      "(Slovenia, plans to begin work on, constitution)\n",
      "(Slovenia, give, full sovereignty)\n",
      "(Slovenia, raised, flag)\n",
      "(Yugoslavia, mobilized troops to regain control, Croatia)\n",
      "(Yugoslavia, normalization of relations, Slovenia)\n",
      "\n",
      "(Recall, measure, content retention)\n",
      "(Compression Ratio, defined as, length of summary/length of original document)\n",
      "(DUC-2001, set, compression lengths)\n",
      "(Overlap judgment, not binary, DUC-2001)\n",
      "(Assessor, judged, system units)\n",
      "\n",
      "(NIST assessors, marked, sharing relations)\n",
      "(McKeown et al., proposed, weighted recall)\n",
      "(weighted recall, treated, completeness of coverage)\n",
      "(system performance, compared, different threshold levels)\n",
      "(DUC-2001, proposed, different versions of weighted recall)\n",
      "\n",
      "(Recall t, defined as, follows)\n",
      "(Weighted retention, defined as, follows)\n",
      "(Unweighted retention, defined as, follows)\n",
      "(Precision, borrowed from, information retrieval research)\n",
      "(Precision, used to measure, how effectively a system generates good summary sentences)\n",
      "\n",
      "(Retention 1, equal to, Precision metric)\n",
      "(Retention w, computed using, formulas)\n",
      "(NeATS, consistently ranked among, top 3)\n",
      "(NeATS, performance for, pseudo precision)\n",
      "(humans, score better than, any system)\n",
      "\n",
      "(NIST assessors, wrote, two separate summaries)\n",
      "(Table 1, includes, pseudo precision)\n",
      "(Table 1, includes, unweighted retention)\n",
      "(Table 1, includes, weighted retention)\n",
      "\n",
      "(page_label, is, 6)\n",
      "(file_name, is, P02-1058.pdf)\n",
      "(87%, is, 47.16%)\n",
      "(48.96%, is, 35.53%)\n",
      "(N*, is, 58.72%)\n",
      "\n",
      "(Philz, founded in, 1982)\n",
      "(page_label, is, 6)\n",
      "(file_name, is, P02-1058.pdf)\n",
      "(S, is, 52.53%)\n",
      "(M, is, 43.39%)\n",
      "\n",
      "(page_label, is, 6)\n",
      "(file_name, is, P02-1058.pdf)\n",
      "(93%, is, 28.31%)\n",
      "(27.01%, is, 15.46%)\n",
      "(U, is, 23.88%)\n",
      "\n",
      "(NeATS, performed better on, longer summaries)\n",
      "(NeATS, is sentence extraction-based, nature)\n",
      "(System Y, was best in, shorter summaries)\n",
      "(Table 2, shows macro-averaged scores for, humans, baselines, and 12 systems)\n",
      "(Table 2, assigns score of, 4 to all, 3 to most, 2 to some, 1 to hardly any, and 0 to none)\n",
      "\n",
      "(Most systems, scored well in, grammaticality)\n",
      "(NeATS, did not fare badly in, quality measures)\n",
      "(Humans, scored, highest in coherence)\n",
      "(Lead baseline, did well in, cohesion)\n",
      "(Coverage baseline, did poorly in, cohesion)\n",
      "\n",
      "(NeATS, fared, quality measures)\n",
      "(NeATS, employed strategies, stigma word filtering)\n",
      "(NeATS, employed strategies, adding lead sentence)\n",
      "(NeATS, employed strategies, time annotation)\n",
      "(Table 2, averaged, grammaticality, cohesion, coherence)\n",
      "\n",
      "(NeATS, uses, simple methods)\n",
      "(NeATS, reducing redundancy, MMR)\n",
      "(NeATS, presenting summary sentences, chronological order)\n",
      "(NeATS, improving capability, content selection)\n",
      "(NeATS, enhancing cohesion, discourse processing)\n",
      "\n",
      "(Goldstein et al., published, paper)\n",
      "(Lin & Hovy, presented, paper)\n",
      "(Lin, developed, Summary Evaluation Environment)\n",
      "(Luhn, published, paper)\n",
      "(Mani et al., published, paper)\n",
      "\n",
      "(McKeown et al., presented at, NAACL-2001 Workshop)\n",
      "(McKeown et al., presented at, DUC-01 Workshop)\n",
      "(Radev & McKeown, published in, Computational Linguistics)\n",
      "(Radev & McKeown, published in, 1998)\n",
      "(Radev & McKeown, title of, Generating Natural Language Summaries from Multiple On-line Sources)\n",
      "\n",
      "(Multi-Document Summarization, builds on, single-document summarization methods)\n",
      "(Multi-Document Summarization, differs from, single-document summarization)\n",
      "(Multi-Document Summarization, addresses, compression, speed, redundancy and passage selection)\n",
      "(Multi-Document Summarization, uses, domain-independent techniques)\n",
      "(Multi-Document Summarization, uses, modular framework)\n",
      "\n",
      "(IR, has not been integrated with, summarization)\n",
      "(summarization system, has greater functionality challenges, in true IR context)\n",
      "(user, issues, search query)\n",
      "(multi-document summarization, should contain, key shared relevant information)\n",
      "(multi-document summarization, has four significant differences)\n",
      "\n",
      "(group of topically-related articles, has, higher degree of redundancy)\n",
      "(temporal dimension, typical in, stream of news reports)\n",
      "(compression ratio, smaller for, collections of dozens or hundreds of topically related documents)\n",
      "(co-reference problem, presents greater challenges for, multi-document summarization)\n",
      "\n",
      "(document summarization, requires, selection/evaluation/ordering/aggregation of items)\n",
      "(text-span deletion, attempts to delete, less important spans)\n",
      "(IBM, developed, automated document summarization)\n",
      "(discourse structure, investigated for, summarization)\n",
      "(machine learning, used to find, patterns in text)\n",
      "\n",
      "(Stein et al., 1999, includes, comparing templates)\n",
      "(TIPSTER, 1998b, finding, co-reference chains)\n",
      "(Mani and Bloedern, 1997, building, activation networks)\n",
      "(McKeown et al., 1999, creates, multi-document summary)\n",
      "(Columbia University system, uses, machine learning)\n",
      "\n",
      "(Multi-Document Summarization, requires, two types of situations)\n",
      "(Multi-Document Summarization, requires, elimination of redundancy)\n",
      "(Multi-Document Summarization, requires, user information seeking goals)\n",
      "(Multi-Document Summarization, requires, interface for system)\n",
      "(Multi-Document Summarization, requires, relevance feedback mechanism)\n",
      "\n",
      "(clustering, ability to, find related information)\n",
      "(coverage, ability to, find main points)\n",
      "(anti-redundancy, ability to, minimize redundancy)\n",
      "\n",
      "(summary cohesion criteria, include, document ordering)\n",
      "(summary cohesion criteria, include, news-story principle)\n",
      "(summary cohesion criteria, include, topic-cohesion)\n",
      "(summary cohesion criteria, include, time line ordering)\n",
      "(effective user interfaces, include, attributability)\n",
      "\n",
      "(user, should be able to, highlight certain parts)\n",
      "(multi-document summarizers, depend on, user's information seeking goals)\n",
      "(multi-document summarizers, create, summaries)\n",
      "(multi-document summarizers, create, overview of pointers)\n",
      "(multi-document summarizers, create, combination of two)\n",
      "\n",
      "(Common Sections, have in common, relevant parts)\n",
      "(Unique Sections, have, relevant parts)\n",
      "(Time Weighting Factor, weights, information)\n",
      "(Summary Extracts, involve, natural language processing)\n",
      "(Textwise, approach, multi-document summary)\n",
      "\n",
      "(MMR-MD, is, metric)\n",
      "(MMR-MD, maximizes, marginal relevance)\n",
      "(MMR-MD, includes, cosine similarity metric)\n",
      "(MMR-MD, includes, coverage score)\n",
      "(MMR-MD, measures, relevance and novelty)\n",
      "\n",
      "(Sire2, uses, cosine similarity metric)\n",
      "(Sire2, penalizes, passages from clusters)\n",
      "(MMR-MD, computes, relevance-ranked list)\n",
      "(MMR-MD, computes, maximal diversity ranking)\n",
      "(MMR-MD, optimizes, linear combination)\n",
      "\n",
      "(SMART, used to compute, cosine similarities)\n",
      "(MMR-MD, used as, summarization metric)\n",
      "(TIPSTER, provided, topical clusters)\n",
      "(apartheid-related news-wire documents, span, 1988-1992)\n",
      "(TIPSTER, provided, topic description)\n",
      "\n",
      "(TIPSTER, provided, topic description)\n",
      "(200 documents, were on average, 31 sentences in length)\n",
      "(6115 sentences, used as, summary unit)\n",
      "(summary, generated, 10 sentences long)\n",
      "\n",
      "(Sire1, similarity metric for, relevance ranking)\n",
      "(Sim2, anti-redundancy metric for, relevance ranking)\n",
      "(R, subset of, passages)\n",
      "(R\\S, set difference of, passages)\n",
      "(timestamp, difference of, documents)\n",
      "\n",
      "(page_label, is, 5)\n",
      "(file_name, is, W00-0405.pdf)\n",
      "(R, contains, passages)\n",
      "(7vw, is subset of, clusters)\n",
      "(7~, is subset of, clusters)\n",
      "\n",
      "(EW. de Klerk, proposed to repeal, apartheid)\n",
      "(Nelson Mandela, called on, international community)\n",
      "(Canadian anti-apartheid groups, urged, government)\n",
      "(South Africa, has, seven major military bases in Angola)\n",
      "(Angola, accused, South Africa of illegal occupation)\n",
      "\n",
      "(ANC, fighting to topple, South African government)\n",
      "(South African government, policy of, apartheid)\n",
      "(South African government, controls, economy)\n",
      "(SWAPO, fighting for, independence for Namibia)\n",
      "(Shultz, voiced concerns about, Soviet influence on ANC)\n",
      "\n",
      "(data sets, constructed for, experimental evaluations of multi-document summarization)\n",
      "(standard IR technique, insufficient for, multi-document summarization)\n",
      "(data sets, allow to measure, effects of features on multi-document summarization quality)\n",
      "(data sets, contain, snapshot of an event from multiple sources)\n",
      "(data sets, contain, unfoldment of an event over time)\n",
      "\n",
      "(George Shultz, voiced concerns about, Soviet influence)\n",
      "(African National Congress, use of violence in, struggle against apartheid)\n",
      "(South Africa, wants closed down, ANC military bases)\n",
      "(Pope, should have spoken out more forcefully against, white-minority government's policies of apartheid)\n",
      "(Harare summit, conditions included, removal of all troops from South Africa's black townships)\n",
      "\n",
      "(African National Congress, suspended, armed struggle)\n",
      "(African National Congress, forged, sanctions strategy)\n",
      "(African National Congress, took, tough line)\n",
      "(President de Klerk, proposed, repeal apartheid)\n",
      "(Nelson Mandela, called on, international community)\n",
      "\n",
      "(summarizer #6, compared to, summarizer #1)\n",
      "(summarizer #6, compared to, summarizer #3)\n",
      "(proposed summarizer, builds upon, single-document summarization)\n",
      "(approach, is domain-independent, statistical processing)\n",
      "(approach, maximizes, novelty of information)\n",
      "\n",
      "(Summarization system, based on, sophisticated natural language understanding)\n",
      "(Summarization system, lacks, co-reference resolution)\n",
      "(Future work, integrate, multi-document summarization with clustering)\n",
      "(Future work, investigate, generating coherent temporally based event summaries)\n",
      "(Future work, investigate, interactive interfaces to browse and explore large document sets)\n",
      "\n",
      "(Lexical chains, used for, text summarization)\n",
      "(Boguraev, Chris Kennedy, authored, Salience based content characterization)\n",
      "(Buckley, authored, SMART information retrieval system)\n",
      "(Carbonell, Goldstein, used, MMR reranking)\n",
      "(Goldstein, Kantrowitz, Mittal, Carbonell, authored, Summarizing Text Documents)\n",
      "\n",
      "(Lexical semantics, used in, summarization)\n",
      "(Julian M. Kupiec, Jan Pedersen, Francine Chen, authored, A trainable document summarizer)\n",
      "(P. H. Luhn, authored, Automatic creation of literature abstracts)\n",
      "(Inderjeet Mani, Eric Bloedern, authored, Multi-document summarization by graph search and merging)\n",
      "(Inderjeet Mani, Eric Bloedom, authored, Summarizing similarities and differences among related documents)\n",
      "\n",
      "(Mandar Mitra, Amit Singhal, Chris Buckley), authored, ACL/EACL-97 Workshop paper)\n",
      "(Chris D. Paice, authored, Info. Proc. and Management paper)\n",
      "(Dragomir Radev, Kathy McKeown, authored, Computational Linguistics paper)\n",
      "(Gerald Salton, authored, Journal of American Society for Information Sciences paper)\n",
      "(Gerald Salton, authored, Automatic Text Processing book)\n",
      "\n",
      "(James Shaw, published, Conciseness through aggregation in text generation)\n",
      "(Gees C. Stein, Tomek Strzalkowski, G. Bowden Wise, published, Summarizing Multiple Documents Using Text Extraction and Interactive Clustering)\n",
      "(Tomek Strzalkowski, Jin Wang, Bowden Wise, published, A robust practical text summarization system)\n",
      "(J. I. Tait, published, Automatic Summarizing of English Texts)\n",
      "(Simone Teufel, Marc Moens, published, Sentence extraction as a classification task)\n",
      "\n",
      "(page_label, is, 9)\n",
      "(file_name, is, W00-0405.pdf)\n",
      "(SIGIR'98, held in, 1998)\n",
      "(Yiming Yang, et al., published, Learning approaches for topic detection and tracking news events)\n",
      "(IEEE Intelligent Systems, published, Special Issue on Applications of Intelligent Information Retrieval)\n"
     ]
    }
   ],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=5,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    # include_embeddings=True,\n",
    "    tags=tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2d3f43b-82d9-4530-983d-e11501675b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/abhinavvengala/Desktop/KG\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89255ac7-34cb-4b22-b6bc-544e83e76abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index_query_engine = kg_index.as_query_engine(\n",
    "    retriever_mode=\"keyword\",\n",
    "    verbose=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8db6a0e7-c389-4473-ac96-5b2fbcf9ab99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;32mExtracted keywords: ['Requirements', 'Summarization', 'Multi', 'Multi-document', 'document']\n",
      "\u001b[0mWARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> No relationships found, returning nodes found by keywords.\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> No nodes found by keywords, returning empty response.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Multi document summarization requires the ability to analyze and synthesize information from multiple documents in order to create a concise summary. It also requires the ability to identify key concepts and relationships between documents.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_index_query_engine.query(\n",
    "    \"requirements of multi document summarization\"\n",
    ")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81d9202e-a4b5-4692-89a4-4ec9c1b8ee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\"summarization\" :entity{name: \"summarization\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\"summarization\" :entity{name: \"summarization\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(\"summarization\" :entity{name: \"summarization\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   p\n",
       "0  (\"summarization\" :entity{name: \"summarization\"...\n",
       "1  (\"summarization\" :entity{name: \"summarization\"...\n",
       "2  (\"summarization\" :entity{name: \"summarization\"..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "USE kap;\n",
    "MATCH p=(n)-[*1..2]-()\n",
    "  WHERE id(n) IN ['multi', 'document', 'summarization', 'multi-document']\n",
    "RETURN p LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7d0afe4-479c-49a0-a961-03ae951abd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"nebulagraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x28d630ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<class 'pyvis.network.Network'> |N|=4 |E|=3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ng_draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f39408b-3679-4685-aecc-c9a0237bd9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ngql DROP SPACE kap;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ff97f-37a2-4be0-ae89-ff6fc891cb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
